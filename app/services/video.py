import glob
import itertools
import os
import random
import gc
import shutil
from typing import List
from loguru import logger
from moviepy import (
    AudioFileClip,
    ColorClip,
    CompositeAudioClip,
    CompositeVideoClip,
    ImageClip,
    TextClip,
    VideoFileClip,
    afx,
    concatenate_videoclips,
)
from moviepy.video.tools.subtitles import SubtitlesClip
from PIL import ImageFont

from app.models import const
from app.models.schema import (
    MaterialInfo,
    VideoAspect,
    VideoConcatMode,
    VideoParams,
    VideoTransitionMode,
    VideoTheme,
)
from app.services.utils import video_effects
from app.utils import utils

# GPUç¼–ç å™¨ç¼“å­˜ï¼ˆé¿å…é‡å¤æ£€æµ‹ï¼‰
_gpu_encoder_cache = None

def detect_gpu_encoder():
    """
    è‡ªåŠ¨æ£€æµ‹GPUç¼–ç å™¨ï¼Œä¼˜å…ˆä½¿ç”¨ç¡¬ä»¶åŠ é€Ÿ
    è¿”å›: (video_codec, extra_ffmpeg_params)
    """
    global _gpu_encoder_cache
    
    # ä½¿ç”¨ç¼“å­˜ç»“æœ
    if _gpu_encoder_cache is not None:
        return _gpu_encoder_cache
    
    import subprocess
    import platform
    import shutil
    
    # é¦–å…ˆæ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨
    ffmpeg_path = shutil.which('ffmpeg')
    if not ffmpeg_path:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°ffmpegå‘½ä»¤ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ffmpegå¹¶æ·»åŠ åˆ°ç³»ç»ŸPATH")
        logger.info("æç¤ºï¼šmacOSå¯ä½¿ç”¨ 'brew install ffmpeg' å®‰è£…")
        # é»˜è®¤ä½¿ç”¨CPUç¼–ç 
        _gpu_encoder_cache = ('libx264', ['-preset', 'ultrafast', '-crf', '23'])
        return _gpu_encoder_cache
    
    try:
        # æ£€æŸ¥ffmpegæ”¯æŒçš„ç¼–ç å™¨
        result = subprocess.run(
            [ffmpeg_path, '-hide_banner', '-encoders'],
            capture_output=True,
            text=True,
            timeout=5
        )
        encoders = result.stdout.lower()
        
        system = platform.system()
        
        # macOS - VideoToolbox (è‹¹æœèŠ¯ç‰‡åŸç”Ÿæ”¯æŒ)
        if system == 'Darwin' and 'h264_videotoolbox' in encoders:
            logger.info("âš¡ GPUåŠ é€Ÿï¼šæ£€æµ‹åˆ° VideoToolbox ç¼–ç å™¨ (macOS)")
            _gpu_encoder_cache = ('h264_videotoolbox', [
                '-allow_sw', '1',  # å¦‚æœç¡¬ä»¶ä¸å¯ç”¨ï¼Œå…è®¸å›é€€åˆ°è½¯ä»¶ç¼–ç 
                '-b:v', '5M',
            ])
            return _gpu_encoder_cache
        
        # NVIDIA NVENC
        if 'h264_nvenc' in encoders or 'nvenc' in encoders:
            logger.info("âš¡ GPUåŠ é€Ÿï¼šæ£€æµ‹åˆ° NVIDIA NVENC ç¼–ç å™¨")
            _gpu_encoder_cache = ('h264_nvenc', [
                '-preset', 'p4',  # p1-p7ï¼Œp4å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡
                '-b:v', '5M',
            ])
            return _gpu_encoder_cache
        
        # AMD AMF
        if 'h264_amf' in encoders or 'amf' in encoders:
            logger.info("âš¡ GPUåŠ é€Ÿï¼šæ£€æµ‹åˆ° AMD AMF ç¼–ç å™¨")
            _gpu_encoder_cache = ('h264_amf', [
                '-quality', 'speed',
                '-b:v', '5M',
            ])
            return _gpu_encoder_cache
        
        # Intel QSV
        if 'h264_qsv' in encoders or 'qsv' in encoders:
            logger.info("âš¡ GPUåŠ é€Ÿï¼šæ£€æµ‹åˆ° Intel QSV ç¼–ç å™¨")
            _gpu_encoder_cache = ('h264_qsv', [
                '-preset', 'veryfast',
                '-b:v', '5M',
            ])
            return _gpu_encoder_cache
        
        logger.info("â„¹ï¸ æœªæ£€æµ‹åˆ°GPUç¼–ç å™¨ï¼Œä½¿ç”¨CPUè½¯ç¼–ç ï¼ˆæ€§èƒ½è¾ƒæ…¢ä½†å…¼å®¹æ€§å¥½ï¼‰")
        
    except subprocess.TimeoutExpired:
        logger.warning("æ£€æµ‹GPUç¼–ç å™¨è¶…æ—¶ï¼Œä½¿ç”¨CPUè½¯ç¼–ç ")
    except Exception as e:
        logger.warning(f"æ£€æµ‹GPUç¼–ç å™¨æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨CPUè½¯ç¼–ç ")
    
    # é»˜è®¤ä½¿ç”¨CPUç¼–ç 
    _gpu_encoder_cache = ('libx264', ['-preset', 'ultrafast', '-crf', '23'])
    return _gpu_encoder_cache


def get_optimal_threads():
    """
    è·å–æœ€ä¼˜çº¿ç¨‹æ•°ï¼šCPUæ ¸å¿ƒæ•° - 1ï¼Œç•™ä¸€ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
    """
    import multiprocessing
    cpu_count = multiprocessing.cpu_count()
    optimal = max(2, cpu_count - 1)
    logger.info(f"ğŸ’» CPUæ ¸å¿ƒæ•°: {cpu_count}ï¼Œä½¿ç”¨çº¿ç¨‹æ•°: {optimal}")
    return optimal


class SubClippedVideoClip:
    def __init__(self, file_path, start_time=None, end_time=None, width=None, height=None, duration=None):
        self.file_path = file_path
        self.start_time = start_time
        self.end_time = end_time
        self.width = width
        self.height = height
        if duration is None:
            self.duration = end_time - start_time
        else:
            self.duration = duration

    def __str__(self):
        return f"SubClippedVideoClip(file_path={self.file_path}, start_time={self.start_time}, end_time={self.end_time}, duration={self.duration}, width={self.width}, height={self.height})"


audio_codec = "aac"
video_codec = "libx264"
fps = 30

def close_clip(clip):
    if clip is None:
        return
        
    try:
        # close main resources
        if hasattr(clip, 'reader') and clip.reader is not None:
            clip.reader.close()
            
        # close audio resources
        if hasattr(clip, 'audio') and clip.audio is not None:
            if hasattr(clip.audio, 'reader') and clip.audio.reader is not None:
                clip.audio.reader.close()
            del clip.audio
            
        # close mask resources
        if hasattr(clip, 'mask') and clip.mask is not None:
            if hasattr(clip.mask, 'reader') and clip.mask.reader is not None:
                clip.mask.reader.close()
            del clip.mask
            
        # handle child clips in composite clips
        if hasattr(clip, 'clips') and clip.clips:
            for child_clip in clip.clips:
                if child_clip is not clip:  # avoid possible circular references
                    close_clip(child_clip)
            
        # clear clip list
        if hasattr(clip, 'clips'):
            clip.clips = []
            
    except Exception as e:
        logger.error(f"failed to close clip: {str(e)}")
    
    del clip
    gc.collect()

def delete_files(files: List[str] | str):
    if isinstance(files, str):
        files = [files]
        
    for file in files:
        try:
            os.remove(file)
        except:
            pass


def _generate_video_from_single_image(
    image_path: str,
    audio_duration: float,
    output_path: str,
    video_width: int,
    video_height: int,
    threads: int = 2,
    enable_animation: bool = False
) -> str:
    """
    ä»å•ä¸€å›¾ç‰‡ç›´æ¥ç”Ÿæˆè§†é¢‘ï¼Œå¯é€‰ç¼©æ”¾åŠ¨ç”»æ•ˆæœ
    ä½¿ç”¨ä¼˜åŒ–çš„ç¼–ç å‚æ•°ä»¥æå‡é€Ÿåº¦
    """
    logger.info(f"generating video from single image: {image_path}")
    logger.info(f"  - target resolution: {video_width}x{video_height}")
    logger.info(f"  - duration: {audio_duration:.2f}s")
    logger.info(f"  - animation: {'enabled' if enable_animation else 'disabled'}")
    
    try:
        # åˆ›å»ºå›¾ç‰‡å‰ªè¾‘ï¼Œè®¾ç½®æ—¶é•¿ä¸ºéŸ³é¢‘æ—¶é•¿
        clip = ImageClip(image_path).with_duration(audio_duration).with_position("center")
        
        # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸
        img_width, img_height = clip.size
        logger.info(f"  - source image size: {img_width}x{img_height}")
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        img_ratio = img_width / img_height
        video_ratio = video_width / video_height
        
        # æ ¹æ®å¼€å…³å†³å®šæ˜¯å¦åº”ç”¨ç¼©æ”¾æ•ˆæœ
        if enable_animation:
            # åº”ç”¨ç¼©æ”¾æ•ˆæœï¼šä»100%ç¼“æ…¢æ”¾å¤§åˆ°120%
            zoom_factor = 1.2
            zoom_clip = clip.resized(lambda t: 1 + (zoom_factor - 1) * (t / audio_duration))
            logger.info(f"  - zoom animation enabled (100% -> 120%)")
        else:
            # ä¸åº”ç”¨ç¼©æ”¾æ•ˆæœï¼Œç›´æ¥ä½¿ç”¨é™æ€å›¾ç‰‡ï¼ˆæ›´å¿«ï¼‰
            zoom_clip = clip
            logger.info(f"  - static image (no animation, faster)")
        
        # å¤„ç†å°ºå¯¸ä¸åŒ¹é…çš„æƒ…å†µ
        if abs(img_ratio - video_ratio) > 0.01:  # æ¯”ä¾‹ä¸åŒ
            logger.info(f"  - image ratio ({img_ratio:.2f}) != video ratio ({video_ratio:.2f}), adding black bars")
            # è®¡ç®—ç¼©æ”¾åçš„å°ºå¯¸
            if img_ratio > video_ratio:
                # å›¾ç‰‡æ›´å®½ï¼Œä»¥å®½åº¦ä¸ºå‡†
                scale_factor = video_width / img_width
                if enable_animation:
                    scale_factor *= 1.2  # ç•™å‡ºç¼©æ”¾ç©ºé—´
            else:
                # å›¾ç‰‡æ›´é«˜ï¼Œä»¥é«˜åº¦ä¸ºå‡†
                scale_factor = video_height / img_height
                if enable_animation:
                    scale_factor *= 1.2  # ç•™å‡ºç¼©æ”¾ç©ºé—´
            
            # åˆ›å»ºé»‘è‰²èƒŒæ™¯
            background = ColorClip(size=(video_width, video_height), color=(0, 0, 0)).with_duration(audio_duration)
            # å°†ç¼©æ”¾åçš„å›¾ç‰‡å±…ä¸­æ”¾ç½®
            final_clip = CompositeVideoClip([background, zoom_clip.with_position("center")])
        else:
            # æ¯”ä¾‹åŒ¹é…ï¼Œç›´æ¥ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸
            logger.info(f"  - image ratio matches video ratio, direct resize")
            final_clip = CompositeVideoClip([zoom_clip.resized((video_width, video_height))])
        
        # ä¼˜åŒ–ç¼–ç å‚æ•°ä»¥æå‡é€Ÿåº¦
        logger.info(f"  - writing video file (optimized encoding)...")
        
        # æ£€æµ‹GPUç¼–ç å™¨
        gpu_codec, gpu_params = detect_gpu_encoder()
        
        # ä½¿ç”¨æ›´å¿«çš„ç¼–ç é¢„è®¾
        output_dir = os.path.dirname(output_path)
        
        # æ„å»ºå®Œæ•´çš„ffmpegå‚æ•°
        ffmpeg_params = gpu_params + [
            '-movflags', '+faststart',  # ä¼˜åŒ–webæ’­æ”¾
        ]
        
        final_clip.write_videofile(
            output_path,
            fps=fps,
            codec=gpu_codec,  # ä½¿ç”¨GPUç¼–ç å™¨
            threads=threads,
            logger=None,
            audio=False,  # ä¸åŒ…å«éŸ³é¢‘
            temp_audiofile_path=output_dir,
            ffmpeg_params=ffmpeg_params
        )
        
        close_clip(clip)
        close_clip(final_clip)
        
        logger.success(f"  âœ“ single image video generated: {output_path}")
        return output_path
        
    except Exception as e:
        logger.error(f"failed to generate video from single image: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

def get_bgm_file(bgm_type: str = "random", bgm_file: str = ""):
    if not bgm_type:
        return ""

    if bgm_file and os.path.exists(bgm_file):
        return bgm_file

    if bgm_type == "random":
        suffix = "*.mp3"
        song_dir = utils.song_dir()
        files = glob.glob(os.path.join(song_dir, suffix))
        return random.choice(files)
    
    if bgm_type == "white_noise":
        # ç”Ÿæˆç™½å™ªéŸ³æ–‡ä»¶
        return _generate_white_noise()

    return ""


def _generate_white_noise(duration=60, sample_rate=44100):
    """
    ç”Ÿæˆç™½å™ªéŸ³éŸ³é¢‘æ–‡ä»¶
    ä½¿ç”¨FFmpegç”Ÿæˆç™½å™ªéŸ³ï¼Œé¿å…é¢å¤–ä¾èµ–
    
    Args:
        duration: ç™½å™ªéŸ³æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤60ç§’ï¼Œè¶³å¤Ÿå¾ªç¯ä½¿ç”¨
        sample_rate: é‡‡æ ·ç‡
    
    Returns:
        str: ç™½å™ªéŸ³æ–‡ä»¶è·¯å¾„
    """
    output_dir = utils.storage_dir("bgm", create=True)
    white_noise_file = os.path.join(output_dir, "white_noise.mp3")
    
    # å¦‚æœç™½å™ªéŸ³æ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    if os.path.exists(white_noise_file):
        logger.info(f"ğŸµ using existing white noise file: {white_noise_file}")
        return white_noise_file
    
    try:
        import subprocess
        logger.info(f"ğŸµ generating white noise ({duration}s)...")
        
        # ä½¿ç”¨FFmpegç”Ÿæˆç™½å™ªéŸ³
        # anoisesrc æ»¤é•œç”Ÿæˆç™½å™ªéŸ³
        cmd = [
            "ffmpeg",
            "-f", "lavfi",
            "-i", f"anoisesrc=duration={duration}:sample_rate={sample_rate}:amplitude=0.1",
            "-ac", "2",  # ç«‹ä½“å£°
            "-y",
            white_noise_file
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            logger.success(f"âœ… white noise generated: {white_noise_file}")
            return white_noise_file
        else:
            logger.error(f"âŒ failed to generate white noise: {result.stderr}")
            return ""
    except Exception as e:
        logger.error(f"âŒ white noise generation failed: {str(e)}")
        return ""


def combine_videos(
    combined_video_path: str,
    video_paths: List[str],
    audio_file: str,
    video_aspect: VideoAspect = VideoAspect.portrait,
    video_concat_mode: VideoConcatMode = VideoConcatMode.random,
    video_transition_mode: VideoTransitionMode = None,
    max_clip_duration: int = 5,
    threads: int = 2,
    enable_animation: bool = False,
) -> str:
    audio_clip = AudioFileClip(audio_file)
    audio_duration = audio_clip.duration
    logger.info(f"audio duration: {audio_duration} seconds")
    # Required duration of each clip
    req_dur = audio_duration / len(video_paths)
    req_dur = max_clip_duration
    logger.info(f"maximum clip duration: {req_dur} seconds")
    output_dir = os.path.dirname(combined_video_path)

    aspect = VideoAspect(video_aspect)
    video_width, video_height = aspect.to_resolution()
    
    # ä¼˜åŒ–ï¼šæ£€æµ‹åˆ°å•ä¸€é™æ€å›¾ç‰‡èµ„æºæ—¶ï¼Œç›´æ¥ç”Ÿæˆè§†é¢‘è€Œä¸èµ°å¤æ‚æ‹¼æ¥æµç¨‹
    if len(video_paths) == 1:
        single_path = video_paths[0]
        ext = utils.parse_extension(single_path)
        if ext in const.FILE_TYPE_IMAGES:
            logger.info(f"detected single image material, using fast generation path")
            close_clip(audio_clip)
            return _generate_video_from_single_image(
                image_path=single_path,
                audio_duration=audio_duration,
                output_path=combined_video_path,
                video_width=video_width,
                video_height=video_height,
                threads=threads,
                enable_animation=enable_animation
            )

    processed_clips = []
    subclipped_items = []
    video_duration = 0
    for video_path in video_paths:
        clip = VideoFileClip(video_path)
        clip_duration = clip.duration
        clip_w, clip_h = clip.size
        close_clip(clip)
        
        start_time = 0

        while start_time < clip_duration:
            end_time = min(start_time + max_clip_duration, clip_duration)            
            if clip_duration - start_time >= max_clip_duration:
                subclipped_items.append(SubClippedVideoClip(file_path= video_path, start_time=start_time, end_time=end_time, width=clip_w, height=clip_h))
            start_time = end_time    
            if video_concat_mode.value == VideoConcatMode.sequential.value:
                break

    # random subclipped_items order
    if video_concat_mode.value == VideoConcatMode.random.value:
        random.shuffle(subclipped_items)
        
    logger.debug(f"total subclipped items: {len(subclipped_items)}")
    
    # Add downloaded clips over and over until the duration of the audio (max_duration) has been reached
    for i, subclipped_item in enumerate(subclipped_items):
        if video_duration > audio_duration:
            break
        
        logger.debug(f"processing clip {i+1}: {subclipped_item.width}x{subclipped_item.height}, current duration: {video_duration:.2f}s, remaining: {audio_duration - video_duration:.2f}s")
        
        try:
            clip = VideoFileClip(subclipped_item.file_path).subclipped(subclipped_item.start_time, subclipped_item.end_time)
            clip_duration = clip.duration
            # Not all videos are same size, so we need to resize them
            clip_w, clip_h = clip.size
            if clip_w != video_width or clip_h != video_height:
                clip_ratio = clip.w / clip.h
                video_ratio = video_width / video_height
                logger.debug(f"resizing clip, source: {clip_w}x{clip_h}, ratio: {clip_ratio:.2f}, target: {video_width}x{video_height}, ratio: {video_ratio:.2f}")
                
                if clip_ratio == video_ratio:
                    clip = clip.resized(new_size=(video_width, video_height))
                else:
                    if clip_ratio > video_ratio:
                        scale_factor = video_width / clip_w
                    else:
                        scale_factor = video_height / clip_h

                    new_width = int(clip_w * scale_factor)
                    new_height = int(clip_h * scale_factor)

                    background = ColorClip(size=(video_width, video_height), color=(0, 0, 0)).with_duration(clip_duration)
                    clip_resized = clip.resized(new_size=(new_width, new_height)).with_position("center")
                    clip = CompositeVideoClip([background, clip_resized])
                    
            shuffle_side = random.choice(["left", "right", "top", "bottom"])
            if video_transition_mode.value == VideoTransitionMode.none.value:
                clip = clip
            elif video_transition_mode.value == VideoTransitionMode.fade_in.value:
                clip = video_effects.fadein_transition(clip, 1)
            elif video_transition_mode.value == VideoTransitionMode.fade_out.value:
                clip = video_effects.fadeout_transition(clip, 1)
            elif video_transition_mode.value == VideoTransitionMode.slide_in.value:
                clip = video_effects.slidein_transition(clip, 1, shuffle_side)
            elif video_transition_mode.value == VideoTransitionMode.slide_out.value:
                clip = video_effects.slideout_transition(clip, 1, shuffle_side)
            elif video_transition_mode.value == VideoTransitionMode.shuffle.value:
                transition_funcs = [
                    lambda c: video_effects.fadein_transition(c, 1),
                    lambda c: video_effects.fadeout_transition(c, 1),
                    lambda c: video_effects.slidein_transition(c, 1, shuffle_side),
                    lambda c: video_effects.slideout_transition(c, 1, shuffle_side),
                ]
                shuffle_transition = random.choice(transition_funcs)
                clip = shuffle_transition(clip)

            if clip.duration > max_clip_duration:
                clip = clip.subclipped(0, max_clip_duration)
                
            # wirte clip to temp file
            clip_file = f"{output_dir}/temp-clip-{i+1}.mp4"
            
            # æ£€æµ‹GPUç¼–ç å™¨
            gpu_codec, gpu_params = detect_gpu_encoder()
            
            clip.write_videofile(
                clip_file, 
                logger=None, 
                fps=fps, 
                codec=gpu_codec,
                ffmpeg_params=gpu_params
            )
            
            close_clip(clip)
        
            processed_clips.append(SubClippedVideoClip(file_path=clip_file, duration=clip.duration, width=clip_w, height=clip_h))
            video_duration += clip.duration
            
        except Exception as e:
            logger.error(f"failed to process clip: {str(e)}")
    
    # loop processed clips until the video duration matches or exceeds the audio duration.
    if video_duration < audio_duration:
        logger.warning(f"video duration ({video_duration:.2f}s) is shorter than audio duration ({audio_duration:.2f}s), looping clips to match audio length.")
        base_clips = processed_clips.copy()
        for clip in itertools.cycle(base_clips):
            if video_duration >= audio_duration:
                break
            processed_clips.append(clip)
            video_duration += clip.duration
        logger.info(f"video duration: {video_duration:.2f}s, audio duration: {audio_duration:.2f}s, looped {len(processed_clips)-len(base_clips)} clips")
     
    # merge video clips progressively, avoid loading all videos at once to avoid memory overflow
    logger.info("starting clip merging process")
    if not processed_clips:
        logger.warning("no clips available for merging")
        return combined_video_path
    
    # if there is only one clip, use it directly
    if len(processed_clips) == 1:
        logger.info("using single clip directly")
        shutil.copy(processed_clips[0].file_path, combined_video_path)
        delete_files(processed_clips)
        logger.info("video combining completed")
        return combined_video_path
    
    # create initial video file as base
    base_clip_path = processed_clips[0].file_path
    temp_merged_video = f"{output_dir}/temp-merged-video.mp4"
    temp_merged_next = f"{output_dir}/temp-merged-next.mp4"
    
    # copy first clip as initial merged video
    shutil.copy(base_clip_path, temp_merged_video)
    
    # merge remaining video clips one by one
    for i, clip in enumerate(processed_clips[1:], 1):
        logger.info(f"merging clip {i}/{len(processed_clips)-1}, duration: {clip.duration:.2f}s")
        
        try:
            # load current base video and next clip to merge
            base_clip = VideoFileClip(temp_merged_video)
            next_clip = VideoFileClip(clip.file_path)
            
            # merge these two clips
            merged_clip = concatenate_videoclips([base_clip, next_clip])

            # æ£€æµ‹GPUç¼–ç å™¨
            gpu_codec, gpu_params = detect_gpu_encoder()
            
            # save merged result to temp file
            merged_clip.write_videofile(
                filename=temp_merged_next,
                threads=threads,
                logger=None,
                temp_audiofile_path=output_dir,
                audio_codec=audio_codec,
                fps=fps,
                codec=gpu_codec,
                ffmpeg_params=gpu_params
            )
            close_clip(base_clip)
            close_clip(next_clip)
            close_clip(merged_clip)
            
            # replace base file with new merged file
            delete_files(temp_merged_video)
            os.rename(temp_merged_next, temp_merged_video)
            
        except Exception as e:
            logger.error(f"failed to merge clip: {str(e)}")
            continue
    
    # after merging, rename final result to target file name
    os.rename(temp_merged_video, combined_video_path)
    
    # clean temp files
    clip_files = [clip.file_path for clip in processed_clips]
    delete_files(clip_files)
            
    logger.info("video combining completed")
    return combined_video_path


def wrap_text(text, max_width, font="Arial", fontsize=60):
    # Create ImageFont
    font = ImageFont.truetype(font, fontsize)

    def get_text_size(inner_text):
        inner_text = inner_text.strip()
        left, top, right, bottom = font.getbbox(inner_text)
        return right - left, bottom - top

    width, height = get_text_size(text)
    if width <= max_width:
        return text, height

    processed = True

    _wrapped_lines_ = []
    words = text.split(" ")
    _txt_ = ""
    for word in words:
        _before = _txt_
        _txt_ += f"{word} "
        _width, _height = get_text_size(_txt_)
        if _width <= max_width:
            continue
        else:
            if _txt_.strip() == word.strip():
                processed = False
                break
            _wrapped_lines_.append(_before)
            _txt_ = f"{word} "
    _wrapped_lines_.append(_txt_)
    if processed:
        _wrapped_lines_ = [line.strip() for line in _wrapped_lines_]
        result = "\n".join(_wrapped_lines_).strip()
        height = len(_wrapped_lines_) * height
        return result, height

    _wrapped_lines_ = []
    chars = list(text)
    _txt_ = ""
    for word in chars:
        _txt_ += word
        _width, _height = get_text_size(_txt_)
        if _width <= max_width:
            continue
        else:
            _wrapped_lines_.append(_txt_)
            _txt_ = ""
    _wrapped_lines_.append(_txt_)
    result = "\n".join(_wrapped_lines_).strip()
    height = len(_wrapped_lines_) * height
    return result, height


def create_bamboo_scroll_subtitles(
    subtitle_items,
    font_path,
    font_size,
    video_width,
    video_height,
    text_color="#FFD700",
    stroke_color="#8B4513",
    stroke_width=2,
    video_duration=None,
    x_offset=0,
    y_offset=0
):
    """
    åˆ›å»ºç«–ç®€å¼å¤šåˆ—å­—å¹•å¸ƒå±€ï¼ˆå¤ä¹¦å·è½´æ¨¡å¼ï¼‰
    
    ç‰¹ç‚¹ï¼š
    1. ä»å³å‘å·¦æ’åˆ—å¤šåˆ—
    2. æ¯åˆ—ä»ä¸Šåˆ°ä¸‹å¡«å……
    3. æ ¹æ®å±å¹•é«˜åº¦å’Œå­—ä½“å¤§å°è®¡ç®—æ¯åˆ—æœ€å¤§å­—æ•°
    4. è‡ªåŠ¨è®¡ç®—å¯å®¹çº³åˆ—æ•°
    5. ä¸‰è‰²é«˜äº®ï¼šæœªè¯»ï¼ˆç°è‰²ï¼‰ã€æ­£åœ¨è¯»ï¼ˆé‡‘è‰²ï¼‰ã€å·²è¯»ï¼ˆæ£•è‰²ï¼‰
    
    å‚æ•°:
        x_offset: æ°´å¹³åç§»é‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
        y_offset: å‚ç›´åç§»é‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    """
    font_size = int(font_size)
    stroke_width = int(stroke_width)
    
    if video_duration is None:
        video_duration = subtitle_items[-1][0][1] if subtitle_items else 10
    
    # åˆ¤æ–­è§†é¢‘æ–¹å‘ï¼ˆæ ¹æ®å¿«é€Ÿæ¨¡å¼ä¼˜åŒ–ï¼‰
    is_portrait = video_height > video_width  # ç«–å±
    
    if is_portrait:
        # ç«–å±ï¼ˆ9:16ï¼‰ï¼šå­—ä½“æ›´å¤§ï¼Œåˆ—æ•°æ›´å°‘ï¼Œåˆ—é—´è·é€‚ä¸­
        base_left = 0.10 + (x_offset / 100.0)
        base_right = 0.70 + (x_offset / 100.0)
        base_y = 0.12 + (y_offset / 100.0)
        column_spacing_multiplier = 1.5  # åˆ—é—´è·å€æ•°ï¼š1.5å€å­—ä½“å¤§å°
        max_columns = 6  # 6åˆ—
    else:
        # æ¨ªå±ï¼ˆ16:9ï¼‰ï¼šå­—ä½“é€‚ä¸­ï¼Œæ›´å¤šåˆ—ï¼Œåˆ—é—´è·æ›´å°
        base_left = 0.18 + (x_offset / 100.0)
        base_right = 0.80 + (x_offset / 100.0)  # 80%ï¼ˆæ°´å¹³ç¦»æ ‡é¢˜æ›´è¿‘ï¼‰
        base_y = 0.12 + (y_offset / 100.0)
        column_spacing_multiplier = 0.75  # åˆ—é—´è·å€æ•°ï¼š0.75å€å­—ä½“å¤§å°ï¼ˆå‡åŠï¼‰
        max_columns = 15  # 15åˆ—ï¼ˆåˆ—é—´è·å‡åŠåå¯æ”¾æ›´å¤šåˆ—ï¼‰
    
    left_boundary = int(video_width * base_left)   # å·¦è¾¹ç•Œ
    right_boundary = int(video_width * base_right)  # å³è¾¹ç•Œ
    y_start = int(video_height * base_y)            # ä¸Šè¾¹ç•Œ
    
    # è®¡ç®—æ¯åˆ—å¯å®¹çº³çš„æœ€å¤§å­—æ•°ï¼ˆä½¿ç”¨1.4å€å­—ç¬¦é—´è·ï¼‰
    char_spacing = int(font_size * 1.4)
    available_height = video_height * 0.76  # 12%-88%åŒºåŸŸ
    max_chars_per_column = int(available_height / char_spacing)
    
    # è®¡ç®—åˆ—é—´è·ï¼ˆæ ¹æ®è§†é¢‘æ¯”ä¾‹ä½¿ç”¨ä¸åŒçš„å€æ•°ï¼‰
    column_spacing = int(font_size * column_spacing_multiplier)
    
    logger.info(f"ğŸ‹ ç«–ç®€å¸ƒå±€: {'9:16 ç«–å±' if is_portrait else '16:9 æ¨ªå±'}, æ¯åˆ—{max_chars_per_column}å­—, {max_columns}åˆ—, åŒºåŸŸ{left_boundary}-{right_boundary}px")
    
    all_clips = []
    
    # å°†æ‰€æœ‰å­—å¹•æ–‡æœ¬è¿æ¥èµ·æ¥ï¼Œåœ¨æ¯å¥ä¹‹é—´æ·»åŠ ç©ºæ ¼åˆ†éš”
    text_parts = []
    for item in subtitle_items:
        text_parts.append(item[1].strip())
    all_text = " ".join(text_parts)  # ä½¿ç”¨ç©ºæ ¼è¿æ¥æ¯å¥ï¼Œä½œä¸ºåˆ†éš”ç¬¦
    total_chars = len(all_text)
    
    # è®¡ç®—å­—ç¬¦åˆ°æ—¶é—´çš„æ˜ å°„
    char_to_time = {}
    char_index = 0
    for item in subtitle_items:
        start_time, end_time = item[0]
        text = item[1].strip()
        duration = end_time - start_time
        char_duration = duration / len(text) if len(text) > 0 else duration
        
        for i, char in enumerate(text):
            char_start = start_time + i * char_duration
            char_end = char_start + char_duration
            char_to_time[char_index] = (char_start, char_end)
            char_index += 1
        
        # ä¸ºç©ºæ ¼åˆ†éš”ç¬¦åˆ†é…æ—¶é—´ï¼ˆä½¿ç”¨å½“å‰å¥å­çš„ç»“æŸæ—¶é—´ï¼‰
        if char_index < total_chars:  # å¦‚æœè¿˜æœ‰ç©ºæ ¼åˆ†éš”ç¬¦
            char_to_time[char_index] = (end_time, end_time)  # ç©ºæ ¼ä¸æ˜¾ç¤ºï¼Œæ—¶é—´ä¸º0
            char_index += 1
    
    # ä»å³å‘å·¦æ’åˆ—å­—ç¬¦ï¼ˆä½¿ç”¨çº¿æ€§æ’å€¼ç¡®ä¿ç²¾ç¡®è¦†ç›–æ•´ä¸ªåŒºåŸŸï¼‰
    char_index = 0
    for col in range(max_columns):
        if char_index >= total_chars:
            break
        
        # è®¡ç®—å½“å‰åˆ—çš„ x ä½ç½®ï¼ˆä»å³åˆ°å·¦ï¼Œä½¿ç”¨çº¿æ€§æ’å€¼ï¼‰
        if max_columns > 1:
            # çº¿æ€§æ’å€¼ï¼šä»å³(right_boundary)åˆ°å·¦(left_boundary)
            x_position = right_boundary - int((right_boundary - left_boundary) * col / (max_columns - 1))
        else:
            x_position = right_boundary
        
        # å¡«å……å½“å‰åˆ—
        for row in range(max_chars_per_column):
            if char_index >= total_chars:
                break
            
            char = all_text[char_index]
            char_start, char_end = char_to_time[char_index]
            
            # è®¡ç®— y ä½ç½®
            y_position = y_start + row * char_spacing
            
            # ç¡®å®šå­—ç¬¦çŠ¶æ€ï¼šæœªè¯»ï¼ˆç°è‰²ï¼‰ã€æ­£åœ¨è¯»ï¼ˆé‡‘è‰²ï¼‰ã€å·²è¯»ï¼ˆæ£•è‰²ï¼‰
            # æœªè¯»çŠ¶æ€ï¼šä»è§†é¢‘å¼€å§‹åˆ°å½“å‰å­—å¼€å§‹
            unread_clip = TextClip(
                text=char,
                font=font_path,
                font_size=font_size,
                color="#000000",  # é»‘è‰²
                stroke_color=stroke_color,
                stroke_width=stroke_width,
            )
            unread_clip = unread_clip.with_start(0).with_duration(char_start)
            unread_clip = unread_clip.with_position((x_position, y_position))
            if char_start > 0:
                all_clips.append(unread_clip)
            
            # æ­£åœ¨è¯»çŠ¶æ€ï¼šå½“å‰å­—æ­£åœ¨æœ—è¯»æ—¶
            reading_clip = TextClip(
                text=char,
                font=font_path,
                font_size=int(font_size * 1.1),  # ç•¥å¾®æ”¾å¤§
                color="#FFD700",  # é‡‘è‰²é«˜äº®
                stroke_color="#8B4513",  # æ£•è‰²æè¾¹
                stroke_width=stroke_width,
            )
            reading_clip = reading_clip.with_start(char_start).with_duration(char_end - char_start)
            reading_clip = reading_clip.with_position((x_position, y_position))
            all_clips.append(reading_clip)
            
            # å·²è¯»çŠ¶æ€ï¼šå½“å‰å­—è¯»å®Œåˆ°è§†é¢‘ç»“æŸ
            read_clip = TextClip(
                text=char,
                font=font_path,
                font_size=font_size,
                color="#8B4513",  # æ£•è‰²
                stroke_color="#FFD700",  # é‡‘è‰²æè¾¹
                stroke_width=stroke_width,
            )
            read_clip = read_clip.with_start(char_end).with_duration(video_duration - char_end)
            read_clip = read_clip.with_position((x_position, y_position))
            if char_end < video_duration:
                all_clips.append(read_clip)
            
            char_index += 1
    
    logger.success(f"âœ… ç«–ç®€å­—å¹•ç”Ÿæˆå®Œæˆ: {len(all_clips)} ä¸ªclip, {char_index} ä¸ªå­—ç¬¦")
    return all_clips


def create_accumulated_subtitles_for_book_theme(subtitle_items, font_path, font_size, 
                                                 video_width, video_height, theme,
                                                 text_color="#000000", stroke_color="#FFFFFF", 
                                                 stroke_width=2, video_duration=None,
                                                 subtitle_x_offset=0, subtitle_y_offset=0):
    """
    ä¸ºä¹¦ç±ä¸»é¢˜åˆ›å»ºè¿½åŠ æ˜¾ç¤ºçš„å­—å¹•ï¼Œå½“æ»¡å±åæ¸…ç©ºç»§ç»­æ˜¾ç¤º
    
    Args:
        subtitle_x_offset: å­—å¹•æ°´å¹³åç§»é‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
        subtitle_y_offset: å­—å¹•å‚ç›´åç§»é‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    """
    font_size = int(font_size)
    stroke_width = int(stroke_width)
    
    # è®¡ç®—è§†é¢‘æ€»æ—¶é•¿ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªå­—å¹•çš„ç»“æŸæ—¶é—´ï¼‰
    if video_duration is None:
        video_duration = subtitle_items[-1][0][1] if subtitle_items else 10
    
    all_clips = []
    
    if theme == VideoTheme.ancient_scroll.value:
        # å¤ä¹¦å·è½´ï¼šä½¿ç”¨ç«–ç®€å¼å¤šåˆ—å¸ƒå±€
        # ä½¿ç”¨ä¼ å…¥çš„åç§»é‡å‚æ•°
        return create_bamboo_scroll_subtitles(
            subtitle_items=subtitle_items,
            font_path=font_path,
            font_size=font_size,
            video_width=video_width,
            video_height=video_height,
            text_color=text_color,
            stroke_color=stroke_color,
            stroke_width=stroke_width,
            video_duration=video_duration,
            x_offset=subtitle_x_offset,
            y_offset=subtitle_y_offset
        )
    else:  # modern_book
        # ç°ä»£å›¾ä¹¦ï¼šæ¨ªæ’è¿½åŠ 
        x_start = int(video_width * 0.1)
        y_start = int(video_height * 0.3)  # ä»30%å¼€å§‹ï¼Œç•™å‡ºæ ‡é¢˜ç©ºé—´
        line_height = int(font_size * 1.5)
        max_lines_per_screen = int((video_height * 0.6) / line_height)  # æ¯å±æœ€å¤šè¡Œæ•°
        max_width = int(video_width * 0.8)
        
        accumulated_lines = []
        page_start_time = 0
        
        for idx, item in enumerate(subtitle_items):
            start_time, end_time = item[0]
            text = item[1].strip()
            
            # è®¡ç®—ä¸‹ä¸€ä¸ªå­—å¹•çš„å¼€å§‹æ—¶é—´ï¼ˆç”¨äºè®¾ç½®å½“å‰clipçš„ç»“æŸæ—¶é—´ï¼‰
            next_start_time = subtitle_items[idx + 1][0][0] if idx + 1 < len(subtitle_items) else video_duration
            
            # æ·»åŠ å½“å‰æ–‡æœ¬åˆ°ç´¯ç§¯è¡Œ
            accumulated_lines.append((text, start_time, end_time, next_start_time))
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¿»é¡µ
            if len(accumulated_lines) > max_lines_per_screen:
                # æ¸…ç©ºå½“å‰é¡µï¼Œå¼€å§‹æ–°é¡µ
                accumulated_lines = [(text, start_time, end_time, next_start_time)]
                page_start_time = start_time
            
            # åˆ›å»ºå½“å‰é¡µé¢æ‰€æœ‰è¡Œçš„clips
            for line_idx, (line_text, line_start, line_end, line_next_start) in enumerate(accumulated_lines):
                y_position = int(y_start + line_idx * line_height)
                
                # å½“å‰æ­£åœ¨æ˜¾ç¤ºçš„è¡Œä½¿ç”¨é»‘è‰²ï¼Œå·²æ˜¾ç¤ºçš„è¡Œä½¿ç”¨ç°è‰²
                if line_start == start_time:
                    # å½“å‰è¡Œï¼šé»‘è‰²
                    line_color = "#000000"
                else:
                    # ä¹‹å‰çš„è¡Œï¼šæ·±ç°è‰²
                    line_color = "#404040"
                
                # è‡ªåŠ¨æ¢è¡Œ
                wrapped_text, _ = wrap_text(
                    line_text,
                    max_width=max_width,
                    font=font_path,
                    fontsize=font_size
                )
                
                line_clip = TextClip(
                    text=wrapped_text,
                    font=font_path,
                    font_size=font_size,
                    color=line_color,
                    stroke_color=stroke_color,
                    stroke_width=stroke_width,
                )
                
                # ä»è¯¥è¡Œå¼€å§‹æ˜¾ç¤ºåˆ°ä¸‹ä¸€æ®µè½å¼€å§‹
                line_clip = line_clip.with_start(line_start)
                line_clip = line_clip.with_duration(line_next_start - line_start)
                line_clip = line_clip.with_position((x_start, y_position))
                all_clips.append(line_clip)
    
    return all_clips


def create_vertical_text_clips(text, font_path, font_size, video_width, video_height, 
                               start_time, end_time, text_color="#FFFFFF", 
                               stroke_color="#000000", stroke_width=2):
    """
    åˆ›å»ºç«–æ’å­—å¹•ï¼Œç”¨äºå¤ä¹¦å·è½´æ¨¡å¼
    å­—ç¬¦é€ä¸ªç«–æ’æ˜¾ç¤ºï¼Œå¹¶åœ¨è¯»åˆ°æ—¶é«˜äº®
    """
    chars = list(text.strip())
    char_clips = []
    
    # ç¡®ä¿å‚æ•°ä¸ºæ•´æ•°
    font_size = int(font_size)
    stroke_width = int(stroke_width)
    
    # è®¡ç®—æ€»æ—¶é•¿å’Œæ¯ä¸ªå­—çš„æ˜¾ç¤ºæ—¶é—´
    total_duration = end_time - start_time
    char_duration = total_duration / len(chars) if len(chars) > 0 else total_duration
    
    # è®¡ç®—ç«–æ’å­—å¹•çš„ä½ç½®ï¼ˆå³ä¾§ï¼Œç•™å‡ºç©ºé—´ç»™æ ‡é¢˜ï¼‰
    x_position = int(video_width * 0.75)  # åœ¨å³ä¾§å››åˆ†ä¹‹ä¸‰å¤„
    y_start = int(video_height * 0.15)  # ä»é¡¶éƒ¨15%å¼€å§‹
    
    for i, char in enumerate(chars):
        char_start = start_time + i * char_duration
        char_end = char_start + char_duration
        
        # ä¸ºæ¯ä¸ªå­—åˆ›å»ºä¸¤ä¸ªçŠ¶æ€ï¼šæ™®é€šå’Œé«˜äº®
        # æ™®é€šçŠ¶æ€ï¼šç™½è‰²
        normal_clip = TextClip(
            text=char,
            font=font_path,
            font_size=font_size,
            color=text_color,
            stroke_color=stroke_color,
            stroke_width=stroke_width,
        )
        
        # é«˜äº®çŠ¶æ€ï¼šé‡‘è‰²
        highlight_clip = TextClip(
            text=char,
            font=font_path,
            font_size=int(font_size * 1.1),  # ç•¥å¾®æ”¾å¤§
            color="#FFD700",  # é‡‘è‰²
            stroke_color="#8B4513",  # æ£•è‰²æè¾¹
            stroke_width=stroke_width,
        )
        
        # è®¡ç®—yä½ç½®
        y_position = int(y_start + i * (font_size + 10))
        
        # æ™®é€šçŠ¶æ€æ˜¾ç¤ºåœ¨æ•´ä¸ªå­—å¹•æœŸé—´
        normal_clip = normal_clip.with_start(start_time).with_duration(total_duration)
        normal_clip = normal_clip.with_position((x_position, y_position))
        
        # é«˜äº®çŠ¶æ€åªåœ¨è¯»åˆ°è¿™ä¸ªå­—æ—¶æ˜¾ç¤º
        highlight_clip = highlight_clip.with_start(char_start).with_duration(char_duration)
        highlight_clip = highlight_clip.with_position((x_position, y_position))
        
        char_clips.append(normal_clip)
        char_clips.append(highlight_clip)
    
    return char_clips


def create_title_clips_for_theme(theme, title_text, font_path, video_width, video_height, 
                                  video_duration, base_font_size=60, stroke_width=2, 
                                  title_x_offset=0, title_y_offset=0):
    """
    æ ¹æ®ä¸»é¢˜åˆ›å»ºæ ‡é¢˜æ–‡æœ¬å—
    
    å‚æ•°:
        title_x_offset: æ ‡é¢˜æ°´å¹³åç§»é‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
        title_y_offset: æ ‡é¢˜å‚ç›´åç§»é‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    """
    # ç¡®ä¿å‚æ•°ä¸ºæ•´æ•°
    base_font_size = int(base_font_size)
    stroke_width = int(stroke_width)
    
    if theme == VideoTheme.cinema.value:
        # ç”µå½±æ¨¡å¼ï¼šå¼€å¤´å…¨å±æ˜¾ç¤º3ç§’ï¼Œå±…ä¸­ï¼Œå¤§å­—ä½“
        title_font_size = int(base_font_size * 2.5)
        title_stroke_width = int(stroke_width * 2)
        
        # è‡ªåŠ¨æ¢è¡Œ
        max_title_width = video_width * 0.8
        wrapped_title, title_height = wrap_text(
            title_text,
            max_width=max_title_width,
            font=font_path,
            fontsize=title_font_size
        )
        
        title_clip = TextClip(
            text=wrapped_title,
            font=font_path,
            font_size=title_font_size,
            color="#FFFFFF",
            stroke_color="#000000",
            stroke_width=title_stroke_width,
        )
        
        # å¼€å¤´æ˜¾ç¤º3ç§’ï¼Œå±…ä¸­
        title_clip = title_clip.with_duration(3)
        title_clip = title_clip.with_start(0)
        title_clip = title_clip.with_position(("center", "center"))
        
        return [title_clip]
        
    elif theme == VideoTheme.ancient_scroll.value:
        # å¤ä¹¦å·è½´ï¼šå³ä¾§ç«–æ’ï¼Œå‚ç›´å±…ä¸­ï¼Œå…¨ç¨‹æ˜¾ç¤º
        # åº”ç”¨æ°´å¹³å’Œå‚ç›´åç§»é‡
        title_font_size = int(base_font_size * 1.2)
        title_stroke_width = int(stroke_width * 1.5)
        
        # å°†æ ‡é¢˜æ–‡å­—ç«–æ’
        chars = list(title_text)
        char_clips = []
        
        # åº”ç”¨åç§»é‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
        base_x = 0.85 + (title_x_offset / 100.0)  # 85%ä½ç½® + åç§»
        
        x_position = int(video_width * base_x)
        
        # è®¡ç®—æ ‡é¢˜æ€»é«˜åº¦å¹¶å‚ç›´å±…ä¸­
        char_height = title_font_size + 5
        title_height = len(chars) * char_height
        y_start = int((video_height - title_height) / 2) + int(video_height * (title_y_offset / 100.0))  # å‚ç›´å±…ä¸­ + åç§»
        
        logger.info(f"ğŸ‹ å¤ä¹¦å·è½´æ ‡é¢˜: X={base_x*100:.1f}%, Y=å‚ç›´å±…ä¸­")
        
        for i, char in enumerate(chars):
            char_clip = TextClip(
                text=char,
                font=font_path,
                font_size=title_font_size,
                color="#8B4513",  # æ£•è‰²ï¼Œå¤ä¹¦æ•ˆæœ
                stroke_color="#FFD700",  # é‡‘è‰²æè¾¹
                stroke_width=title_stroke_width,
            )
            
            y_position = int(y_start + i * char_height)
            char_clip = char_clip.with_duration(video_duration)
            char_clip = char_clip.with_start(0)
            char_clip = char_clip.with_position((x_position, y_position))
            char_clips.append(char_clip)
        
        return char_clips
        
    elif theme == VideoTheme.minimal.value:
        # ç®€çº¦æ¨¡å¼ï¼šå±…ä¸­é ä¸Šï¼Œå…¨ç¨‹æ˜¾ç¤º
        title_font_size = int(base_font_size * 1.8)
        title_stroke_width = int(stroke_width * 1.5)
        
        max_title_width = video_width * 0.8
        wrapped_title, title_height = wrap_text(
            title_text,
            max_width=max_title_width,
            font=font_path,
            fontsize=title_font_size
        )
        
        title_clip = TextClip(
            text=wrapped_title,
            font=font_path,
            font_size=title_font_size,
            color="#FFFFFF",
            stroke_color="#000000",
            stroke_width=title_stroke_width,
        )
        
        title_clip = title_clip.with_duration(video_duration)
        title_clip = title_clip.with_start(0)
        # é¡¶éƒ¨10%å¤„
        title_clip = title_clip.with_position(("center", int(video_height * 0.1)))
        
        return [title_clip]
        
    else:  # modern_book æˆ–é»˜è®¤
        # ç°ä»£å›¾ä¹¦æ¨¡å¼ï¼šé¡¶éƒ¨å±…ä¸­ï¼ˆä¹¦çš®ï¼‰ï¼Œå…¨ç¨‹æ˜¾ç¤º
        title_font_size = int(base_font_size * 1.5)
        title_stroke_width = int(stroke_width * 1.5)
        
        max_title_width = video_width * 0.8
        wrapped_title, title_height = wrap_text(
            title_text,
            max_width=max_title_width,
            font=font_path,
            fontsize=title_font_size
        )
        
        title_clip = TextClip(
            text=wrapped_title,
            font=font_path,
            font_size=title_font_size,
            color="#000000",  # é»‘è‰²æ ‡é¢˜
            stroke_color="#FFFFFF",  # ç™½è‰²æè¾¹
            stroke_width=title_stroke_width,
        )
        
        title_clip = title_clip.with_duration(video_duration)
        title_clip = title_clip.with_start(0)
        # é¡¶éƒ¨20%å¤„
        title_clip = title_clip.with_position(("center", int(video_height * 0.2)))
        
        return [title_clip]


def generate_video(
    video_path: str,
    audio_path: str,
    subtitle_path: str,
    output_file: str,
    params: VideoParams,
):
    aspect = VideoAspect(params.video_aspect)
    video_width, video_height = aspect.to_resolution()

    logger.info(f"generating video: {video_width} x {video_height}")
    logger.info(f"  â‘  video: {video_path}")
    logger.info(f"  â‘¡ audio: {audio_path}")
    logger.info(f"  â‘¢ subtitle: {subtitle_path}")
    logger.info(f"  â‘£ output: {output_file}")

    # https://github.com/harry0703/MoneyPrinterTurbo/issues/217
    # PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'final-1.mp4.tempTEMP_MPY_wvf_snd.mp3'
    # write into the same directory as the output file
    output_dir = os.path.dirname(output_file)

    font_path = ""
    if params.subtitle_enabled:
        if not params.font_name:
            params.font_name = "LXGWWenKai-Regular.ttf"
        
        font_path = os.path.join(utils.font_dir(), params.font_name)
        
        # å¦‚æœé»˜è®¤å­—ä½“ä¸å­˜åœ¨ï¼Œä½¿ç”¨å¤‡ç”¨å­—ä½“
        if not os.path.exists(font_path):
            fallback_fonts = [
                "STHeitiMedium.ttc",
                "MicrosoftYaHeiNormal.ttc",
                "STHeitiLight.ttc",
            ]
            for fallback in fallback_fonts:
                fallback_path = os.path.join(utils.font_dir(), fallback)
                if os.path.exists(fallback_path):
                    logger.warning(f"font {params.font_name} not found, using fallback: {fallback}")
                    font_path = fallback_path
                    params.font_name = fallback
                    break
        
        if os.name == "nt":
            font_path = font_path.replace("\\", "/")

        logger.info(f"  â‘¤ font: {font_path}")
    
    # å¦‚æœæ²¡æœ‰å­—ä½“è·¯å¾„ä½†æœ‰è§†é¢‘æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“
    if not font_path and params.video_subject:
        params.font_name = "LXGWWenKai-Regular.ttf"
        font_path = os.path.join(utils.font_dir(), params.font_name)
        
        # å¦‚æœé»˜è®¤å­—ä½“ä¸å­˜åœ¨ï¼Œä½¿ç”¨å¤‡ç”¨å­—ä½“
        if not os.path.exists(font_path):
            fallback_fonts = ["STHeitiMedium.ttc", "MicrosoftYaHeiNormal.ttc"]
            for fallback in fallback_fonts:
                fallback_path = os.path.join(utils.font_dir(), fallback)
                if os.path.exists(fallback_path):
                    font_path = fallback_path
                    params.font_name = fallback
                    break
        
        if os.name == "nt":
            font_path = font_path.replace("\\", "/")

    def create_text_clip(subtitle_item):
        params.font_size = int(params.font_size)
        params.stroke_width = int(params.stroke_width)
        phrase = subtitle_item[1]
        max_width = video_width * 0.9
        wrapped_txt, txt_height = wrap_text(
            phrase, max_width=max_width, font=font_path, fontsize=params.font_size
        )
        interline = int(params.font_size * 0.25)
        size=(int(max_width), int(txt_height + params.font_size * 0.25 + (interline * (wrapped_txt.count("\n") + 1))))

        _clip = TextClip(
            text=wrapped_txt,
            font=font_path,
            font_size=params.font_size,
            color=params.text_fore_color,
            bg_color=params.text_background_color,
            stroke_color=params.stroke_color,
            stroke_width=params.stroke_width,
            # interline=interline,
            # size=size,
        )
        duration = subtitle_item[0][1] - subtitle_item[0][0]
        _clip = _clip.with_start(subtitle_item[0][0])
        _clip = _clip.with_end(subtitle_item[0][1])
        _clip = _clip.with_duration(duration)
        if params.subtitle_position == "bottom":
            _clip = _clip.with_position(("center", video_height * 0.95 - _clip.h))
        elif params.subtitle_position == "bottom_20":
            # è·ç¦»åº•éƒ¨20%çš„ä½ç½®
            _clip = _clip.with_position(("center", video_height * 0.8 - _clip.h))
        elif params.subtitle_position == "top":
            _clip = _clip.with_position(("center", video_height * 0.05))
        elif params.subtitle_position == "custom":
            # Ensure the subtitle is fully within the screen bounds
            margin = 10  # Additional margin, in pixels
            max_y = video_height - _clip.h - margin
            min_y = margin
            custom_y = (video_height - _clip.h) * (params.custom_position / 100)
            custom_y = max(
                min_y, min(custom_y, max_y)
            )  # Constrain the y value within the valid range
            _clip = _clip.with_position(("center", custom_y))
        else:  # center
            _clip = _clip.with_position(("center", "center"))
        return _clip

    video_clip = VideoFileClip(video_path).without_audio()
    audio_clip = AudioFileClip(audio_path).with_effects(
        [afx.MultiplyVolume(params.voice_volume)]
    )

    def make_textclip(text):
        return TextClip(
            text=text,
            font=font_path,
            font_size=params.font_size,
        )

    if subtitle_path and os.path.exists(subtitle_path):
        logger.info(f"  â‘¥ adding subtitles (theme: {params.video_theme})...")
        sub = SubtitlesClip(
            subtitles=subtitle_path, encoding="utf-8", make_textclip=make_textclip
        )
        text_clips = []
        
        # æ ¹æ®ä¸»é¢˜é€‰æ‹©ä¸åŒçš„å­—å¹•æ ·å¼
        theme = params.video_theme if hasattr(params, 'video_theme') else VideoTheme.modern_book.value
        
        if theme == VideoTheme.ancient_scroll.value or theme == VideoTheme.modern_book.value:
            # å¤ä¹¦å·è½´å’Œç°ä»£å›¾ä¹¦æ¨¡å¼ï¼šä½¿ç”¨è¿½åŠ æ˜¾ç¤ºï¼Œæ»¡å±åç¿»é¡µ
            logger.info(f"  using accumulated subtitle display with page turning")
            
            # è·å–å­—å¹•åç§»é‡å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
            subtitle_x_offset = getattr(params, 'subtitle_x_offset', 0)
            subtitle_y_offset = getattr(params, 'subtitle_y_offset', 0)
            
            # ä½¿ç”¨éŸ³é¢‘æ—¶é•¿ä½œä¸ºvideo_durationï¼Œç¡®ä¿æ‰€æœ‰å­—å¹•éƒ½èƒ½æ˜¾ç¤º
            # å¯¹äºé™æ€å›¾ç‰‡+éŸ³é¢‘ï¼Œvideo_clip.durationå¯èƒ½ä¸å‡†ç¡®ï¼Œaudio_clip.durationæ‰æ˜¯å®Œæ•´æ—¶é•¿
            total_duration = max(audio_clip.duration, video_clip.duration)
            logger.info(f"  total duration: video={video_clip.duration:.2f}s, audio={audio_clip.duration:.2f}s, using={total_duration:.2f}s")
            
            text_clips = create_accumulated_subtitles_for_book_theme(
                subtitle_items=sub.subtitles,
                font_path=font_path,
                font_size=params.font_size,
                video_width=video_width,
                video_height=video_height,
                theme=theme,
                text_color="#000000" if theme == VideoTheme.modern_book.value else params.text_fore_color,
                stroke_color=params.stroke_color,
                stroke_width=params.stroke_width,
                video_duration=total_duration,
                subtitle_x_offset=subtitle_x_offset,
                subtitle_y_offset=subtitle_y_offset
            )
        else:
            # å…¶ä»–æ¨¡å¼ï¼šä½¿ç”¨ä¼ ç»Ÿæ¨ªæ’å­—å¹•
            for item in sub.subtitles:
                clip = create_text_clip(subtitle_item=item)
                text_clips.append(clip)
        
        video_clip = CompositeVideoClip([video_clip, *text_clips])
        logger.success(f"  âœ“ subtitles added ({len(text_clips)} clips)")        
    
    # æ·»åŠ è§†é¢‘æ ‡é¢˜æ˜¾ç¤ºï¼ˆæ ¹æ®ä¸»é¢˜ï¼‰
    if params.video_subject and font_path:
        try:
            theme = params.video_theme if hasattr(params, 'video_theme') else VideoTheme.modern_book.value
            logger.info(f"  â‘¦ adding title: {params.video_subject} (theme: {theme})")
            
            # è·å–æ ‡é¢˜åç§»é‡å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
            title_x_offset = getattr(params, 'title_x_offset', 0)
            title_y_offset = getattr(params, 'title_y_offset', 0)
            
            # ä½¿ç”¨ä¸video_clipç›¸åŒçš„æ—¶é•¿ï¼ˆå·²ç»åŒ…å«äº†å­—å¹•ï¼‰
            current_duration = video_clip.duration
            
            # æ ¹æ®ä¸»é¢˜åˆ›å»ºæ ‡é¢˜
            title_clips = create_title_clips_for_theme(
                theme=theme,
                title_text=params.video_subject,
                font_path=font_path,
                video_width=video_width,
                video_height=video_height,
                video_duration=current_duration,
                base_font_size=params.font_size,
                stroke_width=params.stroke_width,
                title_x_offset=title_x_offset,
                title_y_offset=title_y_offset
            )
            
            # å°†æ ‡é¢˜å åŠ åˆ°è§†é¢‘ä¸Š
            video_clip = CompositeVideoClip([video_clip, *title_clips])
            
            logger.success(f"  âœ“ title added successfully ({len(title_clips)} clips, theme: {theme})")
        except Exception as e:
            logger.error(f"failed to add title: {str(e)}")
            import traceback
            traceback.print_exc()

    bgm_file = get_bgm_file(bgm_type=params.bgm_type, bgm_file=params.bgm_file)
    if bgm_file:
        try:
            logger.info(f"  â‘¦ adding background music: {os.path.basename(bgm_file)}")
            bgm_clip = AudioFileClip(bgm_file).with_effects(
                [
                    afx.MultiplyVolume(params.bgm_volume),
                    afx.AudioFadeOut(3),
                    afx.AudioLoop(duration=video_clip.duration),
                ]
            )
            audio_clip = CompositeAudioClip([audio_clip, bgm_clip])
            logger.success(f"  âœ“ background music added")
        except Exception as e:
            logger.error(f"failed to add bgm: {str(e)}")
    
    logger.info(f"  â‘§ starting final video encoding (this may take a while)...")
    video_clip = video_clip.with_audio(audio_clip)
    
    import time
    encode_start = time.time()
    
    # æ£€æµ‹GPUç¼–ç å™¨
    gpu_codec, gpu_params = detect_gpu_encoder()
    
    # ä½¿ç”¨æœ€ä¼˜çº¿ç¨‹æ•°
    optimal_threads = params.n_threads if params.n_threads else get_optimal_threads()
    
    # æ„å»ºå®Œæ•´çš„ffmpegå‚æ•°
    ffmpeg_params = gpu_params + [
        '-movflags', '+faststart',
    ]
    
    video_clip.write_videofile(
        output_file,
        audio_codec=audio_codec,
        codec=gpu_codec,  # ä½¿ç”¨GPUç¼–ç å™¨
        temp_audiofile_path=output_dir,
        threads=optimal_threads,
        logger=None,
        fps=fps,
        ffmpeg_params=ffmpeg_params
    )
    
    encode_time = time.time() - encode_start
    logger.success(f"  âœ“ final video encoding completed in {encode_time:.1f}s")
    
    video_clip.close()
    del video_clip


def preprocess_video(materials: List[MaterialInfo], clip_duration=4):
    if not materials:
        logger.warning("no materials provided for preprocessing")
        return []
    
    # ä¼˜åŒ–ï¼šå¦‚æœåªæœ‰ä¸€ä¸ªå›¾ç‰‡ç´ æï¼Œä¸éœ€è¦é¢„å¤„ç†ï¼Œç›´æ¥è¿”å›
    # å°†åœ¨combine_videosä¸­ç›´æ¥ç”Ÿæˆè§†é¢‘ï¼Œé¿å…ä¸å¿…è¦çš„è½¬æ¢
    if len(materials) == 1:
        material = materials[0]
        ext = utils.parse_extension(material.url)
        if ext in const.FILE_TYPE_IMAGES:
            logger.info(f"detected single image material, skipping preprocessing for optimization")
            # éªŒè¯å›¾ç‰‡å°ºå¯¸
            try:
                clip = ImageClip(material.url)
                width, height = clip.size
                close_clip(clip)
                if width < 480 or height < 480:
                    logger.warning(f"low resolution material: {width}x{height}, minimum 480x480 required")
                    return []
                logger.success(f"single image material validated: {width}x{height}")
                return materials  # ç›´æ¥è¿”å›åŸå§‹å›¾ç‰‡è·¯å¾„
            except Exception as e:
                logger.error(f"failed to validate image: {str(e)}")
                return []
    
    # å¤šä¸ªç´ ææˆ–éå›¾ç‰‡ç´ æï¼Œèµ°åŸæœ‰é€»è¾‘
    for material in materials:
        if not material.url:
            continue

        ext = utils.parse_extension(material.url)
        try:
            clip = VideoFileClip(material.url)
        except Exception:
            clip = ImageClip(material.url)

        width = clip.size[0]
        height = clip.size[1]
        if width < 480 or height < 480:
            logger.warning(f"low resolution material: {width}x{height}, minimum 480x480 required")
            continue

        if ext in const.FILE_TYPE_IMAGES:
            logger.info(f"processing image: {material.url}")
            # Create an image clip and set its duration to 3 seconds
            clip = (
                ImageClip(material.url)
                .with_duration(clip_duration)
                .with_position("center")
            )
            # Apply a zoom effect using the resize method.
            # A lambda function is used to make the zoom effect dynamic over time.
            # The zoom effect starts from the original size and gradually scales up to 120%.
            # t represents the current time, and clip.duration is the total duration of the clip (3 seconds).
            # Note: 1 represents 100% size, so 1.2 represents 120% size.
            zoom_clip = clip.resized(
                lambda t: 1 + (clip_duration * 0.03) * (t / clip.duration)
            )

            # Optionally, create a composite video clip containing the zoomed clip.
            # This is useful when you want to add other elements to the video.
            final_clip = CompositeVideoClip([zoom_clip])

            # Output the video to a file.
            video_file = f"{material.url}.mp4"
            
            # æ£€æµ‹GPUç¼–ç å™¨
            gpu_codec, gpu_params = detect_gpu_encoder()
            
            final_clip.write_videofile(
                video_file, 
                fps=30, 
                logger=None,
                codec=gpu_codec,
                ffmpeg_params=gpu_params
            )
            close_clip(clip)
            material.url = video_file
            logger.success(f"image processed: {video_file}")
    return materials